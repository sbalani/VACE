# ── cog.yaml ───────────────────────────────────────────────────────────────────
build:
  gpu: true
  cuda: "12.4.1"
  python_version: "3.11"

  # System packages you need for build (e.g. CUDA toolkit, ffmpeg, etc.)
  system_packages:
    - git
    - nvidia-cuda-toolkit
    - ffmpeg
    - libsm6
    - libxext6
    - libxrender-dev
    - libglib2.0-0

  # Stage 1: install "safe" requirements that do NOT pull in flash-attn
  python_requirements: requirements.txt

  # Stage 2: after stage 1 finishes, run these commands in order:
  run:
    # (a) Upgrade pip/setuptools/wheel so build isolation goes smoothly:
    #- "pip install --upgrade pip setuptools wheel"

    # (b) Install build-time dependencies into this same environment:
    #- "pip install packaging ninja torch-utils"

    # (c) Now install flash-attn and its dependents all at once,
    #     without build isolation, so packaging/ninja are available:
    - "pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3+cu124torch2.5-cp311-cp311-linux_x86_64.whl"
    - "pip install wan@git+https://github.com/Wan-Video/Wan2.1.git"

# Your normal predictor entrypoint:
predict: "predict.py:Predictor"
# ───────────────────────────────────────────────────────────────────────────────
